{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear vs logistic regression\n",
    "- linear regression predicts continuous outcome\n",
    "\t- infinite possibilities i.e. negative -∞ - ∞\n",
    "- logistic regression predicts discrete outcome\n",
    "\t- i.e. binary\n",
    "\t- logistic funciton -> sigmoid funciton\n",
    "\n",
    "---\n",
    "### intro to neural networks\n",
    "- neural networks - separate data\n",
    "- classification - create algo to find line that best* separate data. (Least error)\n",
    "\t- linear equations w1x1 + w2x2 + b = 0 or Wx + b = 0\n",
    "\t\t- W = (w1, w2) <- weights\n",
    "\t\t- x = (x1, x2) <- inputs\n",
    "\t\t- b <- bias\n",
    "\t\t- y <- label (0, 1)\n",
    "\t\t- y^ <- prediction\n",
    "\t- goal is to have y == y^\n",
    "- perceptron - linear function + step function\n",
    "\t- linear function - node (inputs) + edges (weights)\n",
    "\t- step function - return binary\n",
    "- **perceptron trick**\n",
    "\t- for a misclassified point, move the line closer\n",
    "\t- take the point and bias multiple by the *learning rate* add/subtract from the line\n",
    "- **error function**\n",
    "    - how far we are from the solution or distance i.e. tell robot to get cake.\n",
    "    - how bad is each point classified\n",
    "        1. Look around\n",
    "        2. Take the step that gets you closer to the objective\n",
    "        3. Repeat\n",
    "        - needs to be continuous not discrete\n",
    "\t- step -> sigmoid function $\\hat{y} = σ(Wx+b)$ aka prediction\n",
    "    - if **correctly** classified - small number\n",
    "    - if **incorrectly** classified - large number \n",
    "    - $$Error(W) = -\\frac{1}{m} \\sum\\limits_{i=1}^{m} y_i \\log(\\hat{y}_i) - (1-y_i) \\log(1-\\hat{y}_i)$$\n",
    "- **softmax function** - multi-class classification, exponent\n",
    "- **one-hot encoding** - a way to label input/output data as vectors instead of actual words i.e. “dog”, “cat”; processing data\n",
    "\t- [One-hot Encoding explained - YouTube](https://www.youtube.com/watch?v=v_4KWmkwmsU)\n",
    "- **probabiliy function** - where the points in the blue region are more likely to be blue and the points in the red region are more likely to be red.\n",
    "- **maximum likelihood** - maximize probability\n",
    "- **cross-entropy** - sum of negative of logarithms of probabilities. ~Relationship between probabilities and error functions~ or ~how likely are the events happen based on the probabilities~ or ~how similar are the vectors~\n",
    "\t- smaller the cross entropy the better\n",
    "\t- goal: minimize cross entropy\n",
    "- **logistic regression**\n",
    "\t- take data\n",
    "\t- pick a random model\n",
    "\t- calculate error\n",
    "\t- minimize error -> get better model\n",
    "\t- repeat\n",
    "- **gradient descent** - misclassified points tell the line to come close vs. correctly classified points tell the line to go farther away\n",
    "\t- small gradient -> change coordinates by a little bit vice versa\n",
    "- **logistic regression/ gradient descent algorithm**\n",
    "    - [DL 32 Gradient Descent Algorithm Fix - YouTube](https://youtu.be/snxmBgi_GeU)\n",
    "- **non-linear models** \n",
    "    - combine linear models, calculate the probability for each, sum them all up and apply the sigmoid function.\n",
    "    - linear combinations * weight + bias\n",
    "- **neural network architecture** - combine perceptrons \n",
    "\t- **multi-class classification**\n",
    "- **feedforward** - turn input into output. Linear combinations\n",
    "- **backpropagation**\n",
    "\t- feedforward\n",
    "\t- compare output of model w/ desired output\n",
    "\t- calculate error\n",
    "\t- run feedforward backwards (backpropagation) updating weights to minimize error\n",
    "\t- keep going until model is good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 4\n",
    "x2 = 5\n",
    "\n",
    "def line(x1, x2):\n",
    "    return 3*x1 + 4*x2 - 10\n",
    "\n",
    "def line2(x1, x2):\n",
    "    return 2.6*x1 + 3.5*x2 - 10.1\n",
    "\n",
    "def line3(x1, x2, bias=1, alpha=0.1):\n",
    "    return (3 - x1 * alpha) *x1 + (4 - x2 * alpha)*x2 - (10 - bias * alpha)\n",
    "\n",
    "print(line(x1, x2))\n",
    "print(line2(x1, x2))\n",
    "print(line3(x1, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line4(x1, x2, bias=1, alpha=0.1):\n",
    "    return (3 + x1 * alpha) *x1 + (4 + x2 * alpha)*x2 - (10 + bias * alpha)\n",
    "\n",
    "x1, x2 = 1, 1\n",
    "print(line(x1, x2))\n",
    "print(line4(x1, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    print(line4(x1, x2, alpha+i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \n",
    "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
    " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
    " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    " 0. 0. 0. 0.]\n",
    "W = [[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "[[0.37454012]\n",
    " [0.95071431]]\n",
    "\n",
    "b = 1.731993941811405\n",
    "1.731993941811405\n",
    "1.731993941811405\n",
    "1.731993941811405\n",
    "1.731993941811405\n",
    "1.731993941811405\n",
    "1.731993941811405\n",
    "1.731993941811405\n",
    "1.731993941811405\n",
    "1.731993941811405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "'''\n",
    "Recall that the perceptron step works as follows. \n",
    "For a point with coordinates (p,q), label y, and prediction given by the equation y^ = step(w1x1 + w2x2 + b) or Wx+b\n",
    "\n",
    "If the point is correctly classified, do nothing. \n",
    "If the point is classified positive, but it has a negative label, change w -> w - αx\n",
    "If the point is classified negative, but it has a positive label, change w -> w + αx\n",
    "'''\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# Perceptron trick - separate data\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i], W, b)\n",
    "        if y[i]-y_hat == 1:\n",
    "            for j in range(len(X[i])):\n",
    "                W[j] += learn_rate * X[i][j]\n",
    "            b += learn_rate\n",
    "        elif y[i]-y_hat == -1:\n",
    "            for j in range(len(X[i])):\n",
    "                W[j] -= learn_rate * X[i][j]\n",
    "            b -= learn_rate\n",
    "    return W, b\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete vs Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def prediction(x):\n",
    "    return sigmoid(4 * x[0] + 5 * x[1] - 9)\n",
    "\n",
    "inputs = [(1,1), (2,4), (5,-5), (-4,5)]\n",
    "for input in inputs:\n",
    "    print(prediction(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# http://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#scalar-operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### neural network\n",
    "\n",
    "learning_rate = 0.1 # alpha\n",
    "weight = 0.5\n",
    "input = 2\n",
    "goal_pred = 0.8\n",
    "\n",
    "for _ in range(20):\n",
    "    pred = input * weight\n",
    "    error = (pred - goal_pred) ** 2\n",
    "    derivative = pred - goal_pred # gradient descent\n",
    "    weight -= learning_rate * derivative\n",
    "    \n",
    "    print(\"Error\", str(error), \"Prediction\", str(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(w1, w2, b):\n",
    "    return w1*0.4 + w2*0.6 + b\n",
    "\n",
    "print(sigmoid(prediction(2,6,-2)))\n",
    "print(sigmoid(prediction(3,5,-2.2)))\n",
    "print(sigmoid(prediction(5,4,-3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
