{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent:  The Code\n",
    "\n",
    "- The sum of the squared error (SSE)\n",
    "    - $ E = \\frac{1}{2} \\sum\\limits_{\\mu} (y^\\mu - \\hat{y}^\\mu)^2$ where $\\mu$ is the data records\n",
    "    - $\\hat{y}$ --> $f(\\sum\\limits_{i} w_iX_i^\\mu)$\n",
    "    - $ E = \\frac{1}{2} \\sum\\limits_{\\mu} (y^\\mu - f(\\sum\\limits_{i} w_iX_i^\\mu))^2$\n",
    "    - error depens on the weight $w_i$ and the input values $x_i$\n",
    "\n",
    "- $Δw_i=\\eta \\delta x_i$\n",
    "- where δ is the error term\n",
    "- $w_i = w_i + \\Delta w_i$ update weight to minimize error\n",
    "- $\\Delta w_i \\propto \\frac{\\partial E}{\\partial w_i}$ -> gradient\n",
    "- $\\Delta w_i \\propto -\\eta \\frac{\\partial E}{\\partial w_i}$ where $\\eta$ is the learning rate\n",
    "\n",
    "- Error Term: $\\delta = (y - \\hat{y}) f'(h)$\n",
    "- $w_i = w_i + \\eta \\delta x_i$\n",
    "- $\\hat{y} = f(h)$ where $h = \\sum\\limits_{i} w_i x_i$\n",
    "- Error Term (expanded): $\\delta = (y - \\hat{y}) \\sum\\limits_{i} w_i x_i$\n",
    "    - $(y - \\hat{y})$ is the output error\n",
    "    - $f'(h)$ - derivative of the activation function -> output gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"\n",
    "    # Derivative of the sigmoid function\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "learnrate = 0.5\n",
    "# input data\n",
    "x = np.array([1, 2, 3, 4])\n",
    "# target\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5, 0.3, 0.1])\n",
    "\n",
    "### Calculate one gradient descent step for each weight\n",
    "### Note: Some steps have been consilated, so there are\n",
    "###       fewer variable names than in the above sample code\n",
    "\n",
    "# TODO: Calculate the node's linear combination of inputs and weights\n",
    "h = np.dot(x, w)\n",
    "\n",
    "# TODO: Calculate output of neural network\n",
    "nn_output = sigmoid(h) #y_hat\n",
    "\n",
    "# TODO: Calculate error of neural network\n",
    "error = y - nn_output\n",
    "\n",
    "# TODO: Calculate the error term\n",
    "#       Remember, this requires the output gradient, which we haven't\n",
    "#       specifically added a variable for.\n",
    "error_term = error * sigmoid_prime(h)\n",
    "\n",
    "# TODO: Calculate change in weights\n",
    "del_w = learnrate * error_term * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Square Error\n",
    "- $E = \\frac{1}{2m}\\sum\\limits_{\\mu}(y^ \\mu - \\hat{y}^ \\mu)^2$\n",
    "    - need a small learning rate, take the average\n",
    "    - if use SSE (sum) will have a large learning rate, gradient descent might diverge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Gradient Descent\n",
    "- update weights: $Δw_{ij}=\\eta * \\delta _j * x_i$\n",
    "- where $\\eta$ - learning rate, $\\delta$ - error, $x_i$ - input values\n",
    "- use gradient descent to train a network on graduate school admissions data (http://www.ats.ucla.edu/stat/data/binary.csv)\n",
    "    - data set has 3 input features: **GRE, GPA, Rank** of prestige of undergraduate school (1-4)\n",
    "    - Goal: predict if a student will be admitted to grad program based on features\n",
    "#### Data cleanup\n",
    "- Use *one-hot encoding* for rank (categorical) -> row of 1's and 0's\n",
    "- Standardize GRE and GPA\n",
    "    - normalization vs. standardization\n",
    "        - normalization: put everything in scale from 0-1\n",
    "        - standardization: turns mean of 0 and std of 1\n",
    "        - standardized value = (X - avg)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
